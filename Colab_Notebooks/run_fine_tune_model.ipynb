{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "run_fine_tune_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dLj6w3H903a",
        "outputId": "5efca7eb-9fc1-4d6a-97c1-653ada0eadbe"
      },
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIl0YUnBnzZi"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7kla3vLnBPF",
        "outputId": "508be15e-e767-4869-ff7b-f78630f9476b"
      },
      "source": [
        "! pip install \"transformers==3.3.0\"\n",
        "! pip install gcsfs\n",
        "! pip install pytrec_eval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/fc/18e56e5b1093052bacf6750442410423f3d9785d14ce4f54ab2ac6b112a6/transformers-3.3.0-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 17.8MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 14.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (20.9)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/26/c02ba92ecb8b780bdae4a862d351433c2912fe49469dac7f87a5c85ccca6/tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 41.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.0) (1.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.0) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.3.0) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=96f1805328aba6893f1eeacca64c750124d04bb6c1cca571e39fe056f21ba338\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.44 sentencepiece-0.1.95 tokenizers-0.8.1rc2 transformers-3.3.0\n",
            "Collecting gcsfs\n",
            "  Downloading https://files.pythonhosted.org/packages/6e/49/2dbc00f89ab9e7513faee7927ea0c649d68eb721108aee860380eaf86ff4/gcsfs-0.8.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from gcsfs) (4.4.2)\n",
            "Collecting ujson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/4e/50e8e4cf5f00b537095711c2c86ac4d7191aed2b4fffd5a19f06898f6929/ujson-4.0.2-cp37-cp37m-manylinux1_x86_64.whl (179kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from gcsfs) (0.4.3)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (1.28.0)\n",
            "Collecting aiohttp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 15.4MB/s \n",
            "\u001b[?25hCollecting fsspec>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/11/f7689b996f85e45f718745c899f6747ee5edb4878cadac0a41ab146828fa/fsspec-0.9.0-py3-none-any.whl (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 38.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gcsfs) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (4.7.2)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (54.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (4.2.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (1.15.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (20.3.0)\n",
            "Requirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (3.0.4)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (3.7.4.3)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 21.9MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 25.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from fsspec>=0.8.0->gcsfs) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth>=1.2->gcsfs) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->fsspec>=0.8.0->gcsfs) (3.4.1)\n",
            "Installing collected packages: ujson, async-timeout, multidict, yarl, aiohttp, fsspec, gcsfs\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-0.9.0 gcsfs-0.8.0 multidict-5.1.0 ujson-4.0.2 yarl-1.6.3\n",
            "Collecting pytrec_eval\n",
            "  Downloading https://files.pythonhosted.org/packages/2e/03/e6e84df6a7c1265579ab26bbe30ff7f8c22745aa77e0799bba471c0a3a19/pytrec_eval-0.5.tar.gz\n",
            "Building wheels for collected packages: pytrec-eval\n",
            "  Building wheel for pytrec-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytrec-eval: filename=pytrec_eval-0.5-cp37-cp37m-linux_x86_64.whl size=264407 sha256=bd162d7b94bbac76f127eaf98cb99532e8dbd0f324cdfcfffed598a896d7df6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/66/40/1779aa0a8eb66e088669befe286f695cdfe420ba91ce662127\n",
            "Successfully built pytrec-eval\n",
            "Installing collected packages: pytrec-eval\n",
            "Successfully installed pytrec-eval-0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzFlwRLsoL8d"
      },
      "source": [
        "**Get the code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw9ykM-6n5IM",
        "outputId": "50363166-acdd-4525-aca6-04721488ecde"
      },
      "source": [
        "import sys\n",
        "%cd /content/\n",
        "!rm -r ExactMatchMarking\n",
        "!test -d ExactMatchMarking || git clone https://github.com/BOUALILILila/ExactMatchMarking.git\n",
        "import sys\n",
        "if not 'ExactMatchMarking' in sys.path:\n",
        "  sys.path += ['ExactMatchMarking']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "rm: cannot remove 'ExactMatchMarking': No such file or directory\n",
            "Cloning into 'ExactMatchMarking'...\n",
            "remote: Enumerating objects: 506, done.\u001b[K\n",
            "remote: Counting objects: 100% (506/506), done.\u001b[K\n",
            "remote: Compressing objects: 100% (340/340), done.\u001b[K\n",
            "remote: Total 506 (delta 356), reused 313 (delta 163), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (506/506), 122.67 KiB | 2.50 MiB/s, done.\n",
            "Resolving deltas: 100% (356/356), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mybi55fAoT7r"
      },
      "source": [
        "**Google Cloud Storage set up**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RCbDvDYx13ZB",
        "outputId": "d9b4f552-1ef0-4f4a-baac-dcb5da192e95"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdUFaYR4wwZv"
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJvKZN02w2j4"
      },
      "source": [
        "import os\n",
        "os.environ['COLAB_SKIP_TPU_AUTH'] = '1'\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FaSBpUMw6dZ",
        "outputId": "c080c231-74ce-41d1-9b62-df3778508d9a"
      },
      "source": [
        "project_ID=\"YOUR GCS PROJECT ID #@param {type:\"string\"}\n",
        "bucket_name=\"BUCKET NAME\" #@param {type:\"string\"}\n",
        "!gcloud config set project {project_ID} "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJ4osMoMo20I"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLEoGaMYyoM_",
        "outputId": "396c621a-7b71-4509-d9bd-3d31cbbbe545"
      },
      "source": [
        "## Tain params\n",
        "# if precise tokens use augmented model \n",
        "MODEL_NAME_OR_PATH = \"google/electra-base-discriminator\" #@param {type:\"string\"}\n",
        "CHECKPOINT_NAME='electra_ckpts' #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "OUTPUT_DIR = \"gs://path/output/dir/to/save/tf_ckpts\" #@param {type:\"string\"}\n",
        "assert OUTPUT_DIR, 'Must specify an existing GCS bucket name'\n",
        "tf.io.gfile.makedirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
        "\n",
        "COLLECTION='msmarco' #@param {type:\"string\"}\n",
        "\n",
        "STRATEGY= \"sim_pair\" #@param {type:\"string\"}\n",
        "\n",
        "  ## Train\n",
        "TRAIN_DATA_DIR = \"gs://path/to/train/dataset/tfrecord\" #@param {type:\"string\"}\n",
        "# Convention: {TRAIN_DATA_DIR}/dataset_train_{TRAIN_SET_NAME}.tf\n",
        "TRAIN_SET_NAME= \"msmarco_small_sim_pair\" #@param {type:\"string\"}\n",
        "\n",
        "# Must be on GCS\n",
        "LOG_DIR = \"gs://path/to/log/dir/on/GCS\"#@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: gs://lila_data/msmarco/electra_base *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EB_cwCer6s7"
      },
      "source": [
        "## Run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjQVRoSA8zVZ",
        "outputId": "a4c6da05-38ab-4cad-c9ba-0c5e135c05ca"
      },
      "source": [
        "import logging \n",
        "import os, glob, re\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from transformers import (\n",
        "    TF2_WEIGHTS_NAME,\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    HfArgumentParser,\n",
        "    PreTrainedTokenizer,\n",
        "    TFAutoModelForSequenceClassification,\n",
        "    TFTrainingArguments,\n",
        ")\n",
        "\n",
        "from Modeling import (\n",
        "    CustomTFTrainer,\n",
        "    CustomTFTrainingArguments,\n",
        "    get_eval_metric,\n",
        "    wrap_relevance_head,\n",
        ")\n",
        "from args import (\n",
        "    ModelArguments,\n",
        "    DataTrainingArguments,\n",
        ")\n",
        "\n",
        "def get_logger(name, level=logging.INFO):\n",
        "    handler = logging.StreamHandler(stream=sys.stderr)\n",
        "    formatter = logging.Formatter(\n",
        "        fmt=\"%(asctime)s: %(funcName)s +%(lineno)s: %(levelname)-8s %(message)s\",\n",
        "        datefmt=\"%H:%M:%S\"\n",
        "    )\n",
        "    handler.setFormatter(formatter)\n",
        "\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(level)\n",
        "    logger.addHandler(handler)\n",
        "    return logger \n",
        "\n",
        "logger = get_logger('Train')\n",
        "\n",
        "\n",
        "training_args = CustomTFTrainingArguments(\n",
        "    output_dir = OUTPUT_DIR,\n",
        "    do_train = True,\n",
        "    do_eval = False,\n",
        "    do_predict = False,\n",
        "    do_early_stopping = False,\n",
        "    evaluate_during_training = False,\n",
        "    per_device_train_batch_size = 16,\n",
        "    per_device_eval_batch_size = 4,\n",
        "    learning_rate= 3e-6,\n",
        "    weight_decay = 0.01,\n",
        "    adam_epsilon = 1e-6,\n",
        "    num_train_epochs = -1,\n",
        "    max_steps = 100000,\n",
        "    warmup_steps = 10000,\n",
        "    logging_steps = 1000,\n",
        "    save_steps = 5000,\n",
        "    tpu_name = f\"grpc://{os.environ['COLAB_TPU_ADDR']}\",\n",
        "    tf_ckpt_dir = f'{OUTPUT_DIR}/{CHECKPOINT_NAME}/{STRATEGY}', # where to save the TF2 pure trainable checkpoints\n",
        "    ckpt_dir = f'/path/to/local/or/drive/{STRATEGY}_wrap', # where to save the Transformers library ckpt format easy to reuse \n",
        "    eval_all_checkpoints = False,\n",
        "    logging_dir = LOG_DIR,\n",
        ")\n",
        "\n",
        "\n",
        "data_args = DataTrainingArguments(\n",
        "    collection = COLLECTION,\n",
        "    how = 'words',\n",
        "    train_data_dir = TRAIN_DATA_DIR,\n",
        "    # eval_data_dir = EVAL_DATA_DIR,\n",
        "    train_set_name = TRAIN_SET_NAME,\n",
        "    # eval_set_name = EVAL_SET_NAME,\n",
        "    # test_set_name = TEST_SET_NAME,\n",
        "    # out_suffix = OUT_SUFFIX,\n",
        "    # eval_qrels_file = EVAL_QRELS_FILE_NAME,\n",
        "    # test_qrels_file = TEST_QRELS_FILE_NAME,\n",
        "    max_seq_length = 512,\n",
        ")\n",
        "\n",
        "model_args = ModelArguments(\n",
        "    model_name_or_path = MODEL_NAME_OR_PATH,\n",
        ")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \n",
        "    logger.info(\n",
        "        \"n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "        training_args.n_gpu,\n",
        "        bool(training_args.n_gpu > 1),\n",
        "        training_args.fp16,\n",
        "    )\n",
        "    logger.info(\"Training/evaluation/prediction parameters %s\", training_args)\n",
        "\n",
        "    if training_args.do_train or training_args.do_eval:\n",
        "       \n",
        "        num_labels = 2\n",
        "        output_mode = 'classification'\n",
        "\n",
        "        # Load pretrained model and tokenizer\n",
        "        #\n",
        "        # Distributed training:\n",
        "        # The .from_pretrained methods guarantee that only one local process can concurrently\n",
        "        # download model & vocab.\n",
        "\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
        "            num_labels=num_labels,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            output_hidden_states=False,\n",
        "        )\n",
        "\n",
        "        with training_args.strategy.scope():\n",
        "            orig_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "                model_args.model_name_or_path,\n",
        "                from_pt=True if glob.glob(f\"{model_args.model_name_or_path}/*.bin\") else False,\n",
        "                config=config,\n",
        "                cache_dir=model_args.cache_dir,\n",
        "            )\n",
        "\n",
        "            orig_classifier = orig_model.classifier\n",
        "            # For ELECTRA need to change the classifier \n",
        "            model = wrap_relevance_head(orig_model)\n",
        "            print('Verfiy wrapping')\n",
        "            print(model.classifier)\n",
        "            print(type(model))\n",
        "\n",
        "\n",
        "        # Get datasets\n",
        "        if training_args.do_train:\n",
        "            filename = os.path.join(data_args.train_data_dir, f'dataset_train_{data_args.train_set_name}.tf')\n",
        "            print('== filename train : ', filename)\n",
        "            if tf.io.gfile.exists(filename):\n",
        "                train_dataset, num_train_examples = data_args.doc_processor.get_train_dataset(filename, \n",
        "                                                                            training_args.train_batch_size, training_args.seed)\n",
        "                print(num_train_examples)\n",
        "            else:\n",
        "                raise IOError('File does not exist: ', filename)\n",
        "        else:\n",
        "            train_dataset, num_train_examples = None, 0\n",
        "\n",
        "        if training_args.do_eval or training_args.do_early_stopping:\n",
        "            filename = os.path.join(data_args.eval_data_dir, f'dataset_dev_{data_args.eval_set_name}.tf')\n",
        "            ids_file = os.path.join(data_args.eval_data_dir, f'query_pass_ids_dev_{data_args.eval_set_name}.tsv')\n",
        "            if tf.io.gfile.exists(filename):\n",
        "                eval_dataset, num_eval_examples = data_args.doc_processor.get_eval_dataset(filename, \n",
        "                                                                        training_args.eval_batch_size)\n",
        "            else:\n",
        "                raise IOError('File does not exist: ', filename)\n",
        "            if tf.io.gfile.exists(ids_file):\n",
        "                query_doc_ids = pd.read_csv(ids_file, \n",
        "                                            header=None, index_col=None, delimiter='\\t', \n",
        "                                            names=['id','qid','did','pass'], \n",
        "                                            dtype={'id':str, 'qid':str,'did':str})\n",
        "            else:\n",
        "                raise IOError('File does not exist: ', ids_file)\n",
        "\n",
        "            if data_args.eval_qrels_file is not None:\n",
        "                qrels_file = os.path.join(data_args.eval_data_dir, f'{data_args.eval_qrels_file}.tsv')\n",
        "                if tf.io.gfile.exists(qrels_file):\n",
        "                    eval_qrels = pd.read_csv(qrels_file,\n",
        "                                header=None, index_col=None, delimiter='\\t', names=['qid','did','label'], \n",
        "                                dtype={'qid':str,'did':str, 'label':int})\n",
        "                else:\n",
        "                    raise IOError('File does not exist: ', qrels_file)\n",
        "            else:\n",
        "                eval_qrels = None\n",
        "        else:\n",
        "            eval_dataset, num_eval_examples, query_doc_ids, eval_qrels = None, 0, None, None\n",
        "\n",
        "        # eval_metric = get_eval_metric(data_args.collection)\n",
        "\n",
        "\n",
        "        # Initialize our Trainer\n",
        "        trainer = CustomTFTrainer(\n",
        "            model = model,\n",
        "            args = training_args,\n",
        "            train_dataset = train_dataset,\n",
        "            num_train_examples = num_train_examples,\n",
        "            eval_dataset = eval_dataset,\n",
        "            num_eval_examples = num_eval_examples,\n",
        "            out_suffix = f'{data_args.eval_set_name}_{data_args.out_suffix}', # eval output file name\n",
        "            # eval_metric = eval_metric,\n",
        "            # compute_metrics = eval_metric.compute_on_df,\n",
        "            query_doc_ids = query_doc_ids,\n",
        "            eval_qrels = eval_qrels,\n",
        "        )\n",
        "\n",
        "        # Training\n",
        "        if training_args.do_train:\n",
        "            trainer.train()\n",
        "\n",
        "            if not training_args.do_early_stopping:\n",
        "                trainer.save_model()\n",
        "                # You can restore the original classifier architecture and change the trained weigths \n",
        "                # to keep a ckpt that can be loaded with the standard TFAutoModelForSequenceClassification (but need to call the wrap_relevance_head)\n",
        "                # Or use the first ckpt saved above with trainer.save_model() that needs to be loaded with the custom TFElectraForRelevanceClassification class\n",
        "                trained_model = trainer.model\n",
        "                orig_classifier.dropout, orig_classifier.out_proj = trained_model.classifier.dropout,  trained_model.classifier.out_proj\n",
        "                trained_model.classifier = orig_classifier\n",
        "                trained_model.save_pretrained(f'/path/local/drive/{STRATEGY}_wrap_unwrap')\n",
        "                \n",
        "\n",
        "        # Evaluation        \n",
        "        if training_args.do_eval:\n",
        "            logger.info(\"*** Evaluate ***\")\n",
        "\n",
        "            sstep = 'final' if training_args.do_train else 0\n",
        "            results = trainer.evaluate(step=sstep)\n",
        "            print(sstep)\n",
        "            print(results)\n",
        "\n",
        "    # Test\n",
        "    if training_args.do_predict:\n",
        "\n",
        "        checkpoints = list(\n",
        "                os.path.dirname(c)\n",
        "                for c in sorted(\n",
        "                    glob.glob(f'{training_args.ckpt_dir}' + \"/**/\" + TF2_WEIGHTS_NAME, recursive=True),\n",
        "                    key=lambda f: int(\"\".join(filter(str.isdigit, f)) or -1),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        if len(checkpoints) == 0:\n",
        "            raise IOError('No checkpoint found at this location: ', training_args.ckpt_dir)\n",
        "\n",
        "        elif not training_args.eval_all_checkpoints:\n",
        "            if training_args.ckpt_dir in checkpoints:\n",
        "                checkpoints = [training_args.ckpt_dir]\n",
        "            else:\n",
        "                checkpoints = [checkpoints[-1]]    \n",
        "                    \n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "\n",
        "        filename = os.path.join(data_args.eval_data_dir, f'dataset_test_{data_args.test_set_name}.tf')\n",
        "        if tf.io.gfile.exists(filename):\n",
        "            test_dataset, num_test_examples = data_args.doc_processor.get_eval_dataset(filename, \n",
        "                                                    training_args.eval_batch_size)\n",
        "        else: \n",
        "            raise IOError('File does not exist: ', filename)\n",
        "\n",
        "        ids_file = os.path.join(data_args.eval_data_dir, f'query_pass_ids_test_{data_args.test_set_name}.tsv')\n",
        "        if tf.io.gfile.exists(ids_file):\n",
        "            query_doc_ids = pd.read_csv(ids_file,\n",
        "                                header=None, index_col=None, delimiter='\\t', \n",
        "                                names=['id','qid','did','pass'],\n",
        "                                dtype={'id':str, 'qid':str,'did':str})\n",
        "        else: \n",
        "            raise IOError('File does not exist: ', ids_file)\n",
        "\n",
        "        if data_args.test_qrels_file:\n",
        "            qrels_file = os.path.join(data_args.eval_data_dir, f'{data_args.test_qrels_file}.tsv')\n",
        "            if tf.io.gfile.exists(qrels_file):\n",
        "                test_qrels = pd.read_csv(qrels_file, \n",
        "                            header=None, index_col=None, delimiter='\\t', \n",
        "                            names=['qid','did','label'], \n",
        "                            dtype={'qid':str,'did':str, 'label':int})\n",
        "                \n",
        "            else :\n",
        "                raise IOError('File does not exist: ', qrels_file)\n",
        "        else:\n",
        "            test_qrels = None\n",
        "\n",
        "        for checkpoint in checkpoints:\n",
        "            global_step = checkpoint.split(\"-\")[-1] if re.match(\".*checkpoint*-[0-9]\", checkpoint) else \"final\"\n",
        "            logger.info(\"Evaluate the following checkpoint-step: %s - %s\", checkpoint, global_step)\n",
        "            print(\"Evaluate the following checkpoint-step:\", checkpoint, global_step)\n",
        "\n",
        "            with training_args.strategy.scope():\n",
        "                # if using the wrap_unwrap ckpt\n",
        "                trained_model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "                trained_model = wrap_relevance_head(trained_model)\n",
        "                # # if using the wrap ckpt ==> modified classifier\n",
        "                # trained_model = TFElectraForRelevanceClassification.from_pretrained(checkpoint)\n",
        "        \n",
        "            trainer = CustomTFTrainer(\n",
        "                          model = trained_model,\n",
        "                          args = training_args,\n",
        "                      )\n",
        "\n",
        "            trainer.predict(test_dataset, num_test_examples, query_doc_ids, test_qrels, \n",
        "                            f'{data_args.test_set_name}_{data_args.out_suffix}-{global_step}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.97.79.42:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ExactMatchMarking/Modeling/training_args.py:108: UserWarning: TrainingArguments: (gs://lila_data/msmarco/electra_base/electra_ckpts/pre_pair_wrap) already exists and is not empty.                 Continuing training from last tf checkpoint if exists.                 Use --overwrite_tf_ckpt_dir to overwrite existing checkpoints (restart training).\n",
            "  Use --overwrite_tf_ckpt_dir to overwrite existing checkpoints (restart training).\"\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/training_args_tf.py:216: FutureWarning: The n_gpu argument is deprecated and will be removed in a future version, use n_replicas instead.\n",
            "  FutureWarning,\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.97.79.42:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
            "16:59:41: main +104: INFO     n_gpu: 8, distributed training: True, 16-bits training: False\n",
            "INFO:Train:n_gpu: 8, distributed training: True, 16-bits training: False\n",
            "16:59:41: main +106: INFO     Training/evaluation/prediction parameters CustomTFTrainingArguments(output_dir='gs://lila_data/msmarco/electra_base', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy='no', prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=3e-06, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-06, max_grad_norm=1.0, num_train_epochs=-1, max_steps=100000, warmup_steps=10000, logging_dir='gs://lila_data/log', logging_first_step=False, logging_steps=1000, save_steps=5000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=16, dataloader_num_workers=0, past_index=-1, run_name=None, disable_tqdm=None, remove_unused_columns=True, label_names=None, tpu_name=None, xla=False, warmup_prop=0.0, patience=20, best_score=None, delta=0, do_early_stopping=False, ckpt_name=None, ckpt_dir='/content/drive/MyDrive/Colab Notebooks/electra_ckpts/pre_pair_wrap', tf_ckpt_dir='gs://lila_data/msmarco/electra_base/electra_ckpts/pre_pair_wrap', overwrite_ckpt_dir=False, overwrite_tf_ckpt_dir=False, max_ckpt_keep=3, save_all_ckpts=False, eval_all_checkpoints=False)\n",
            "INFO:Train:Training/evaluation/prediction parameters CustomTFTrainingArguments(output_dir='gs://lila_data/msmarco/electra_base', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy='no', prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=3e-06, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-06, max_grad_norm=1.0, num_train_epochs=-1, max_steps=100000, warmup_steps=10000, logging_dir='gs://lila_data/log', logging_first_step=False, logging_steps=1000, save_steps=5000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=16, dataloader_num_workers=0, past_index=-1, run_name=None, disable_tqdm=None, remove_unused_columns=True, label_names=None, tpu_name=None, xla=False, warmup_prop=0.0, patience=20, best_score=None, delta=0, do_early_stopping=False, ckpt_name=None, ckpt_dir='/content/drive/MyDrive/Colab Notebooks/electra_ckpts/pre_pair_wrap', tf_ckpt_dir='gs://lila_data/msmarco/electra_base/electra_ckpts/pre_pair_wrap', overwrite_ckpt_dir=False, overwrite_tf_ckpt_dir=False, max_ckpt_keep=3, save_all_ckpts=False, eval_all_checkpoints=False)\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFElectraForSequenceClassification: ['electra.embeddings.position_ids']\n",
            "- This IS expected if you are initializing TFElectraForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFElectraForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFElectraForSequenceClassification for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "verfiy wrapping\n",
            "<Modeling.reRankers.TFElectraRelevanceHead object at 0x7f5ed47cf590>\n",
            "<class 'transformers.modeling_tf_electra.TFElectraForSequenceClassification'>\n",
            "== filename train :  gs://lila_data/msmarco/electra_base/dataset_train_msmarco_small_pre_pair.tf\n",
            "79561622\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Checkpoint file gs://lila_data/msmarco/electra_base/electra_ckpts/pre_pair_wrap/ckpt-11 found and restoring from checkpoint\n",
            "INFO:absl:  Continuing training from checkpoint, will skip to saved global_step\n",
            "INFO:absl:  Continuing training from epoch 0\n",
            "INFO:absl:  Continuing training from global step 55000\n",
            "INFO:absl:  Will skip the first 55000 steps in the 1 epoch\n",
            "INFO:absl:***** Running training *****\n",
            "INFO:absl:  Num examples = 79561622\n",
            "INFO:absl:  Num Epochs = 1\n",
            "INFO:absl:  Total optimization steps = 100000\n",
            "INFO:absl:Starting Epoch 1 ...\n",
            "INFO:absl:Skipping the first 55000 trained steps ...\n",
            "INFO:absl:55000 training steps skipped. Resuming training at step 55000...\n",
            "INFO:absl:Epoch 1 Step 56000 Train Loss 0.2038\n",
            "INFO:absl:Epoch 1 Step 57000 Train Loss 0.2035\n",
            "INFO:absl:Epoch 1 Step 58000 Train Loss 0.1111\n",
            "INFO:absl:Epoch 1 Step 59000 Train Loss 0.2006\n",
            "INFO:absl:Epoch 1 Step 60000 Train Loss 0.1131\n",
            "INFO:absl:Saving checkpoint for step 60000 at gs://lila_data/msmarco/electra_base/electra_ckpts/pre_pair_wrap/ckpt-12\n",
            "INFO:absl:Epoch 1 Step 61000 Train Loss 0.1034\n",
            "INFO:absl:Epoch 1 Step 62000 Train Loss 0.0692\n",
            "INFO:absl:Epoch 1 Step 63000 Train Loss 0.1546\n",
            "INFO:absl:Epoch 1 Step 64000 Train Loss 0.1452\n",
            "INFO:absl:Epoch 1 Step 65000 Train Loss 0.0495\n",
            "INFO:absl:Saving checkpoint for step 65000 at gs://lila_data/msmarco/electra_base/electra_ckpts/pre_pair_wrap/ckpt-13\n",
            "INFO:absl:Epoch 1 Step 66000 Train Loss 0.0721\n",
            "INFO:absl:Epoch 1 Step 67000 Train Loss 0.2287\n",
            "INFO:absl:Epoch 1 Step 68000 Train Loss 0.1363\n",
            "INFO:absl:Epoch 1 Step 69000 Train Loss 0.0490\n",
            "INFO:absl:Epoch 1 Step 70000 Train Loss 0.0977\n",
            "INFO:absl:Saving checkpoint for step 70000 at gs://lila_data/msmarco/electra_base/electra_ckpts/pre_pair_wrap/ckpt-14\n",
            "INFO:absl:Epoch 1 Step 71000 Train Loss 0.1482\n",
            "INFO:absl:Epoch 1 Step 72000 Train Loss 0.1703\n",
            "INFO:absl:Epoch 1 Step 73000 Train Loss 0.2270\n",
            "INFO:absl:Epoch 1 Step 74000 Train Loss 0.0977\n",
            "INFO:absl:Epoch 1 Step 75000 Train Loss 0.1130\n",
            "INFO:absl:Saving checkpoint for step 75000 at gs://lila_data/msmarco/electra_base/electra_ckpts/pre_pair_wrap/ckpt-15\n",
            "INFO:absl:Epoch 1 Step 76000 Train Loss 0.1645\n",
            "INFO:absl:Epoch 1 Step 77000 Train Loss 0.1281\n",
            "INFO:absl:Epoch 1 Step 78000 Train Loss 0.1135\n",
            "INFO:absl:Epoch 1 Step 79000 Train Loss 0.1813\n",
            "INFO:absl:Epoch 1 Step 80000 Train Loss 0.1445\n",
            "INFO:absl:Saving checkpoint for step 80000 at gs://lila_data/msmarco/electra_base/electra_ckpts/pre_pair_wrap/ckpt-16\n",
            "INFO:absl:Epoch 1 Step 81000 Train Loss 0.1182\n",
            "INFO:absl:Epoch 1 Step 82000 Train Loss 0.1865\n",
            "INFO:absl:Epoch 1 Step 83000 Train Loss 0.1265\n",
            "INFO:absl:Epoch 1 Step 84000 Train Loss 0.1024\n",
            "INFO:absl:Epoch 1 Step 85000 Train Loss 0.0939\n",
            "INFO:absl:Saving checkpoint for step 85000 at gs://lila_data/msmarco/electra_base/electra_ckpts/pre_pair_wrap/ckpt-17\n",
            "INFO:absl:Epoch 1 Step 86000 Train Loss 0.1558\n",
            "INFO:absl:Epoch 1 Step 87000 Train Loss 0.1060\n",
            "INFO:absl:Epoch 1 Step 88000 Train Loss 0.1469\n",
            "INFO:absl:Epoch 1 Step 89000 Train Loss 0.1519\n",
            "INFO:absl:Epoch 1 Step 90000 Train Loss 0.0653\n",
            "INFO:absl:Saving checkpoint for step 90000 at gs://lila_data/msmarco/electra_base/electra_ckpts/pre_pair_wrap/ckpt-18\n",
            "INFO:absl:Epoch 1 Step 91000 Train Loss 0.1215\n",
            "INFO:absl:Epoch 1 Step 92000 Train Loss 0.0796\n",
            "INFO:absl:Epoch 1 Step 93000 Train Loss 0.1232\n",
            "INFO:absl:Epoch 1 Step 94000 Train Loss 0.1317\n",
            "INFO:absl:Epoch 1 Step 95000 Train Loss 0.1334\n",
            "INFO:absl:Saving checkpoint for step 95000 at gs://lila_data/msmarco/electra_base/electra_ckpts/pre_pair_wrap/ckpt-19\n",
            "INFO:absl:Epoch 1 Step 96000 Train Loss 0.0920\n",
            "INFO:absl:Epoch 1 Step 97000 Train Loss 0.1553\n",
            "INFO:absl:Epoch 1 Step 98000 Train Loss 0.0594\n",
            "INFO:absl:Epoch 1 Step 99000 Train Loss 0.1275\n",
            "INFO:absl:Epoch 1 Step 100000 Train Loss 0.1291\n",
            "INFO:absl:Saving checkpoint for step 100000 at gs://lila_data/msmarco/electra_base/electra_ckpts/pre_pair_wrap/ckpt-20\n",
            "INFO:absl:Saving model in /content/drive/MyDrive/Colab Notebooks/electra_ckpts/pre_pair_wrap\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}